{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uqa84irXnqZi"
   },
   "source": [
    "# Analyzing A Movie Review Dataset(Part 2)[100 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SWS7b11Vn9dG"
   },
   "source": [
    "## 0-A. Set-up[-2 if author function is not called]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0EIRM9vnvRP"
   },
   "outputs": [],
   "source": [
    "def author(gt_username = 'pbutler33'):\n",
    "    print(\"This assignment is submitted by {0}.\".format(gt_username))\n",
    "#Add your GT_UserName below and uncomment the line.\n",
    "#author()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XJrlQ8Psoagw"
   },
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tOfan1VXoERu"
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awf_c13mR_3A"
   },
   "source": [
    "## 0-B. Text Preprocessing[10(5+5) Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F22uJmPYoFuZ"
   },
   "outputs": [],
   "source": [
    "# Read the train,test and unlabeled data from files and store them into variables train,test and unlabeled_train respectively.\n",
    "### Add your code here. \n",
    "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-Ss_Khqpyso"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def clean_review(review, remove_stopwords = False):\n",
    "    \"\"\"Helper function to clean the reviews i.e. to convert a document to a sequence of words.\n",
    "     Please note that we're not removing stopwords since word2vec relies on the broader context\n",
    "     of the sentence in order to produce high-quality word vectors.\n",
    "\n",
    "     Arg: review: review string (str)\n",
    "          remove_stopwards: If true remove stopwords else not. (boolean)\n",
    "     Returns: cleaned_review : Cleaned review (list)\n",
    "\n",
    "     You should carry out the following steps.\n",
    "     1. Remove HTML Tags.\n",
    "     2. Remove non-letter characters.\n",
    "     3. Convert to lower case.\n",
    "    \"\"\"\n",
    "\n",
    "    ### Add your code here.\n",
    "  \n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    cleaned_review = review_text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        cleaned_review = [w for w in cleaned_review if not w in stops]\n",
    "\n",
    "    #####################\n",
    "    \n",
    "    return cleaned_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yb15ZgwB4RNN"
   },
   "outputs": [],
   "source": [
    "def review_to_sentences(review: str, tokenizer: nltk.tokenize.punkt.PunktSentenceTokenizer, remove_stopwords=False):\n",
    "    \"\"\"Helper function to split a review into parsed sentences. Returns a \n",
    "     list of sentences, where each sentence is a list of words.\n",
    "\n",
    "     Arg: review: review string (str)\n",
    "          tokenizer: punkt tokenizer\n",
    "     Returns:\n",
    "          review_sentences: List of list of tokens.\n",
    "                            e.g. [[\"word2vec\", \"was\", \"introduced\", \"by\", \"google\" ],[\"it\",\"leverages\",\"distributed\",\"token\",\"representations\"]]\n",
    "\n",
    "     You should carry out the following steps.\n",
    "     1. Use the tokenizer to split the paragraph into sentences.\n",
    "     2. Clean the sentence to return a list of words for each sentence using the helper funtion above.\n",
    "     3. Return a list of tokenized sentences.\n",
    "    \"\"\"\n",
    "    ### Add your code here.\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    review_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            review_sentences.append(clean_review(raw_sentence, \\\n",
    "              remove_stopwords))\n",
    "    \n",
    "    ######################\n",
    "    \n",
    "    return review_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xi_1vFZ7-sd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'... ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'.. .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:335: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "# Use the above helper functions to convert the reviews in train[\"review\"] and in unlabeled_train[\"review\"] to a list of list format as mentioned above.\n",
    "# For example if your train data contains 2 reviews with 3 sentences in each and the unlabeled_train has 4 reviews with 1 sentence in each\n",
    "# The resultant list should have 10 lists of tokenized sentences.\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "### Add your code here.\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "######################\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "### Add your code here.\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGfJfIH0SVDD"
   },
   "source": [
    "## 1. Word2Vec[80(20*4) Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74UeOdgiURxZ"
   },
   "source": [
    "### a. Create vector representations for each movie post in your training set by training word2vec with context=5, embedding dimension=100, min_words=40. We’ll call the collection of these representations Z1.[20 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmVoyc32IwsI"
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for s in sentences:\n",
    "    word_list+=s\n",
    "word_list = list(set(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "_39o3jMeGyKV",
    "outputId": "405775a0-82b2-4aff-fac8-dd3faa29f459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "def generate_z1(sentences,num_features,min_word_count,context,num_workers = 4,downsampling = 1e-3, model_name = \"model1_100features_40minwords_5context\"):\n",
    "    \"\"\"Set values for the parameters of the your word2vec model and train it on the extracted sentences from both the train and unlabeled_train data and\n",
    "      generate the collection of these representations Z1.\n",
    "      You should carry out the following.\n",
    "      1) Set the parameters as mentioned below. \n",
    "        A)Constrained Paramters : \"context length\", \"embedding dimension\", \"min_words\" (Please check the question for the values.)\n",
    "        B)Optional Parameters: \"number of workers\", \"downsample setting\"\n",
    "      2) Train your word2vec model and save it.\n",
    "      3) Store the collection of word embeddings and the word_list(z1 and word_list_z1) .\n",
    "\n",
    "      Arg: sentences: List of tokenized sentences (List)\n",
    "            num_features: Word vector dimensionality (int)\n",
    "            min_word_count: Minimum word count (int)\n",
    "            context: Context window size (int)\n",
    "            num_workers: Number of threads to run in parallel (int)\n",
    "            downsampling: Downsample setting for frequent words (float)\n",
    "            model_name = Name to save your model (str)\n",
    "      Returns:\n",
    "            trained_word2vec_model: word2vec model trained on the tokenized sentences.\n",
    "            z1: word embeddings (ndarray)\n",
    "            word_list_z1: List of tokens in the model (List)\n",
    "\n",
    "      \n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Training model...\")\n",
    "    ### Add your code here.\n",
    "    trained_word2vec_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                size=num_features, min_count = min_word_count, \\\n",
    "                window = context, sample = downsampling)\n",
    "    \n",
    "    trained_word2vec_model.init_sims(replace=True)\n",
    "    trained_word2vec_model.save(model_name)\n",
    "    \n",
    "    word_list_z1 = [w for w in trained_word2vec_model.wv.vocab]\n",
    "    \n",
    "    z1 = np.array([trained_word2vec_model[w] for w in word_list_z1])      \n",
    "    \n",
    "    #######################\n",
    "\n",
    "    return trained_word2vec_model, z1, word_list_z1\n",
    "\n",
    "# Set values for the parameters of the your word2vec model and train it on the extracted sentences from both the train and unlabeled_train data and\n",
    "# generate the collection of these representations Z1.\n",
    "# Add the function parameter values below.\n",
    "### Add your code here.\n",
    "context = 5\n",
    "num_features = 100\n",
    "min_word_count = 40\n",
    "######################\n",
    "\n",
    "model1, z1, word_list_z1 = generate_z1(sentences,num_features,min_word_count,context)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hESq0fhVUd7_"
   },
   "source": [
    "### b. Create vector representations for each movie post in your training set by loading the pretrained Google word2vec model. We’ll call the collection of these representations Z2.[20 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3zJqisAnJCtA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "def generate_z2(word_list_z1):\n",
    "    \"\"\"Use a pre-trained word2vec model(GoogleNews-vectors-negative300) to generate the collection of these representations Z2. Please note Z2 corresponds to the word embeddings for\n",
    "      words that are present in both the previous model and this pre-trained model.\n",
    "\n",
    "      You should carry out the following.\n",
    "\n",
    "      Arg: word_list_z1: List of tokens in the previous trained model (List)\n",
    "      Returns:\n",
    "            pre_trained_word2vec_model: word2vec model trained on the tokenized sentences.\n",
    "            z2: word embeddings (ndarray)\n",
    "            word_list_z2: List of tokens in the model (List)   \n",
    "\n",
    "    \"\"\"\n",
    "    ### Add your code here.\n",
    "    pre_trained_word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    \n",
    "    word_list_z2 = [w for w in word_list if w in pre_trained_word2vec_model.wv.vocab] #only look at words in word_list to keep dimensionality lower\n",
    "    \n",
    "    z2 = np.array([pre_trained_word2vec_model[w] for w in word_list_z2]) \n",
    "    \n",
    "    ######################\n",
    "    return pre_trained_word2vec_model, z2, word_list_z2\n",
    "\n",
    "model2, z2, word_list_z2 = generate_z2(word_list_z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x9Ug2cErftjY"
   },
   "source": [
    "### c. With k=10, do k-means clustering on each set Z1, Z2. Print a table of the words in each cluster for Z1 and for Z2.[20 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ks_JRyqJrdX"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5UxPcOVgbCF"
   },
   "outputs": [],
   "source": [
    "def fit_kmeans(z,word_list_z,num_clusters = 10):\n",
    "    \"\"\" Fit kmeans on the embedding representations and return a mapping of word to cluster indices. Please use the default values for\n",
    "        the rest of the kmeans parameters.\n",
    "\n",
    "        Arg: z: word embeddings (ndarray)\n",
    "              word_list_z: List of tokens in the model (List) \n",
    "              num_clusters: Number of clusters (int)\n",
    "        Returns:\n",
    "            pre_trained_word2vec_model: word2vec model trained on the tokenized sentences.\n",
    "            z2: word embeddings (ndarray)\n",
    "            word_centroid_map_z: A mapping of word to cluster index it belongs to. (Dict)      \n",
    "\n",
    "    \"\"\"\n",
    "    ### Add your code here.\n",
    "    kmeans_clustering = KMeans(n_clusters = num_clusters)\n",
    "    idx = kmeans_clustering.fit_predict(z)\n",
    "    word_centroid_map_z = dict(zip(word_list_z, idx))\n",
    "    ######################\n",
    "\n",
    "    return word_centroid_map_z\n",
    "\n",
    "\n",
    "word_centroid_map_z1 = fit_kmeans(z1, word_list_z1, 10)\n",
    "word_centroid_map_z2 = fit_kmeans(z2, word_list_z2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hZt0ZVXeg9nA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The clusters for model1 are....\n",
      "Cluster 0\n",
      "['all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'i', 've', 'started', 'to', 'music', 'watching', 'documentary', 'here', 'and', 'there', 'watched', 'again']\n",
      "Cluster 1\n",
      "['with', 'has', 's', 'starts', 'wants', 'wanted', 'dunno', 'hates', 'came', 'working', 'performing', 'does', 'gave', 'behind', 'goes', 'succeeds', 'doing', 'appreciated', 'comes', 'faithful']\n",
      "Cluster 2\n",
      "['kay', 'michael', 'jackson', 'joe', 'pesci', 'saint', 'timothy', 'hines', 'mr', 'tom', 'nicholas', 'bell', 'robert', 'carradine', 'stacy', 'brian', 'james', 'vanessa', 'david', 'keith']\n",
      "Cluster 3\n",
      "['press', 'drugs', 'drug', 'plans', 'planet', 'war', 'worlds', 'bases', 'investors', 'secret', 'animal', 'dna', 'scientists', 'predators', 'visitors', 'tourists', 'youngsters', 'area', 'security', 'center']\n",
      "Cluster 4\n",
      "['odd', 'cool', 'of', 'subtle', 'towards', 'obvious', 'visually', 'impressive', 'boring', 'nice', 'smooth', 'convincing', 'powerful', 'complex', 'wholesome', 'ironically', 'talented', 'extremely', 'stupid', 'entertaining']\n",
      "Cluster 5\n",
      "['listening', 'want', 'get', 'make', 'remember', 'see', 'hate', 'find', 'call', 'say', 'know', 'have', 'think', 'stay', 'try', 'give', 'be', 'lengths', 'recreate', 'criticize']\n",
      "Cluster 6\n",
      "['insight', 'messages', 'character', 'patience', 'resemblance', 'shortcomings', 'primal', 'nature', 'scientific', 'ambition', 'human', 'historical', 'goods', 'results', 'physical', 'originality', 'journey', 'desire', 'representation', 'justification']\n",
      "Cluster 7\n",
      "['mj', 'his', 'who', 'innocent', 'criminal', 'psychopathic', 'nah', 'buddy', 'girl', 'manager', 'stalking', 'meanwhile', 'agent', 'her', 'mate', 'raising', 'angel', 'belle', 'man', 'crusoe']\n",
      "Cluster 8\n",
      "['dead', 'etc', 'turning', 'car', 'robot', 'speed', 'demon', 'dance', 'closed', 'doors', 'cruise', 'tiger', 'deadly', 'fence', 'creature', 'prey', 'pack', 'fight', 'giant', 'run']\n",
      "The clusters for model2 are....\n",
      "Cluster 0\n",
      "['persian', 'ect', 'robinson', 'ard', 'det', 'kaal', 'nong', 'souza', 'nuit', 'chil', 'graydon', 'nus', 'myers', 'sheffield', 'marxist', 'craig', 'laal', 'gook', 'willis', 'yous']\n",
      "Cluster 1\n",
      "['epochal', 'inexhaustible', 'uncynical', 'conventionalism', 'ferocity', 'reductive', 'pulsating', 'sloppily', 'freakiness', 'valediction', 'irreverent', 'indirectness', 'filmgoing', 'simile', 'retro', 'contorting', 'aphoristic', 'entertainments', 'unpretentiously', 'gentlemanly']\n",
      "Cluster 2\n",
      "['stopping', 'guested', 'pumping', 'crisscross', 'doctored', 'pessimistic', 'startled', 'persuaded', 'weakend', 'balky', 'gliding', 'hounding', 'rake', 'sitting', 'rippled', 'creaked', 'lobbing', 'patted', 'quieting', 'simmering']\n",
      "Cluster 3\n",
      "['causally', 'emit', 'compactor', 'actin', 'lobes', 'coincident', 'centerline', 'technique', 'excrements', 'appendix', 'soy', 'progenitors', 'phonics', 'emitting', 'unnoticeable', 'microwaves', 'equator', 'kinetics', 'pistons', 'axis']\n",
      "Cluster 4\n",
      "['stupidness', 'wannabee', 'frighteners', 'squaddies', 'vag', 'rollin', 'toast', 'superfun', 'toos', 'headbanger', 'freaky', 'piggy', 'mechs', 'teleprompters', 'heebie', 'magnates', 'poser', 'questioningly', 'mooches', 'oompa']\n",
      "Cluster 5\n",
      "['recreational', 'seders', 'airbase', 'ambassador', 'wreck', 'plea', 'complainant', 'honeymooning', 'racecar', 'workhouse', 'homes', 'doga', 'tenement', 'seaters', 'genealogical', 'trolley', 'walkouts', 'beacon', 'cryptologist', 'midnight']\n",
      "Cluster 6\n",
      "['folding', 'masonry', 'yarn', 'shoebox', 'stools', 'bustiers', 'dyed', 'headpiece', 'sculpting', 'imprinted', 'jobbie', 'kibbee', 'wristwatch', 'sampler', 'papercuts', 'fountain', 'bandanna', 'cedar', 'exterior', 'mehndi']\n",
      "Cluster 7\n",
      "['unloving', 'embolden', 'unceasingly', 'pittance', 'unfriendly', 'outwardly', 'cruel', 'epistles', 'antifeminist', 'undignified', 'seriousness', 'redeeming', 'ragbag', 'provocation', 'blindly', 'signally', 'disrespectfully', 'trustful', 'extraneous', 'lionising']\n",
      "Cluster 8\n",
      "['decides', 'blinks', 'dampens', 'reaches', 'impersonates', 'implores', 'obfuscates', 'ruminates', 'depresses', 'botches', 'falls', 'roams', 'surfs', 'turns', 'threatens', 'trembles', 'scrambles', 'feeds', 'falsifies', 'prepares']\n"
     ]
    }
   ],
   "source": [
    "def print_clusters(word_centroid_map_z, model_name):\n",
    "    \"\"\" Print max(20, cluster_size) words for each of the clusters.\n",
    "\n",
    "        Args: word_centroid_map_z: A mapping of word to cluster index it belongs to. (Dict)  \n",
    "              model_name: Model Name (str)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"The clusters for {0} are....\".format(model_name))\n",
    "    ### Add your code here.\n",
    "    for i in range(max(word_centroid_map_z.values())):\n",
    "        print(\"Cluster {0}\".format(i))\n",
    "        cluster = [k for k in word_centroid_map_z.keys() if word_centroid_map_z[k] == i]\n",
    "        if len(cluster)>20:\n",
    "            cluster=cluster[:20]\n",
    "        print(cluster)\n",
    "    ######################\n",
    "print_clusters(word_centroid_map_z1, \"model1\")\n",
    "print_clusters(word_centroid_map_z2, \"model2\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab_type": "text",
    "id": "dVtybcB0i7er"
   },
   "source": [
    "### d. Featurize the training and test reviews in Z1, Z2 to produce design matrices X1,X2 as described in part 3 of the blog series. Basically, each review is converted into a bag of centroids feature vector with each vector component representing the count of the number of words in that review that belong in that component’s cluster.[20 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4cGF5_EZjDUs"
   },
   "outputs": [],
   "source": [
    "# Create tokenized reviews for both train and test. Please note that for this we'll not be splitting the sentences and we'll be removing stopwords.\n",
    "# This is exactly what you did in the preprocessing step for HW1\n",
    "clean_train_reviews = []\n",
    "clean_test_reviews = []\n",
    "### Add your code here.\n",
    "for r in train.review:\n",
    "    clean_train_reviews.append(clean_review(r, remove_stopwords=True))\n",
    "for r in test.review:\n",
    "    clean_test_reviews.append(clean_review(r, remove_stopwords=True))\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGuPb-Hplp5A"
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(tokenized_review, word_centroid_map, num_clusters = 10):\n",
    "    \"\"\" Create a bag of kmeans centroids for each review i.e. for a review we return an array of length num_clusters with each \n",
    "        element of the array indicating how many words(tokens) of the review belong to that cluster.\n",
    "\n",
    "        Args: tokenized_review: list of tokens corresponding to a review (List)\n",
    "              word_centroid_map: word to cluster_index map for the model (Dict)\n",
    "              num_clusters: Number of clusters (int)\n",
    "        \n",
    "        Returns: bag_of_centroids: An array containing the count of tokens in each cluster (ndarray)\n",
    "    \"\"\"\n",
    "  \n",
    "    ### Add your code here.\n",
    "    bag_of_centroids = np.zeros(num_clusters)\n",
    "    for w in tokenized_review:\n",
    "        if w in word_centroid_map:\n",
    "            bag_of_centroids[word_centroid_map[w]] = bag_of_centroids[word_centroid_map[w]]+1\n",
    "            \n",
    "    #######################\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EImSAI6LvkeS"
   },
   "outputs": [],
   "source": [
    "def create_design_matrices(data, cleaned_reviews, word_centroid_map_z1, word_centroid_map_z2, num_clusters = 10):\n",
    "    \"\"\" Creates the design matrices X1(for trained model) and X2(for pretrained google word2vec model) for the given data\n",
    "\n",
    "        Args: data: Train/Test data (pandas.core.frame.DataFrame)\n",
    "              cleaned_reviews: List of tokenized reviews(sentences are not split and stopwords removed) (List)\n",
    "              word_centroid_map_z1: word to cluster map for model1 (Dict)\n",
    "              word_centroid_map_z2: word to cluster map for model2 (Dict)\n",
    "              num_clusters: Number of KMeans clusters (int)\n",
    "\n",
    "        Returns:\n",
    "              x1_data: Design matrices X1-- Shape should be num_reviews*num_clusters (np.ndarray) \n",
    "              x2_data: Design matrices X2-- Shape should be num_reviews*num_clusters (np.ndarray)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ### Add your code here.\n",
    "    \n",
    "    x1_data = np.array([create_bag_of_centroids(r, word_centroid_map_z1, num_clusters) for r in cleaned_reviews])\n",
    "    x2_data = np.array([create_bag_of_centroids(r, word_centroid_map_z2, num_clusters) for r in cleaned_reviews])\n",
    "    \n",
    "    ######################\n",
    "    return x1_data,x2_data\n",
    "x1_train,x2_train = create_design_matrices(train,clean_train_reviews,word_centroid_map_z1,word_centroid_map_z2,10)\n",
    "x1_test,x2_test = create_design_matrices(test,clean_test_reviews,word_centroid_map_z1,word_centroid_map_z2,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wPzXOj2ryAQW"
   },
   "source": [
    "### e. Save X1, X2 for both train and test set.[-10 if missing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VOFv1becyFDs"
   },
   "outputs": [],
   "source": [
    "#Save the design matrices for both train and test set as .npy files and submit these as part of the delieverables\n",
    "### Add your code here.\n",
    "np.save('x1_train', x1_train)\n",
    "np.save('x2_train', x2_train)\n",
    "np.save('x1_test', x1_test)\n",
    "np.save('x2_test', x2_test)\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCTKysWnwA_C"
   },
   "source": [
    "## 2. Classification Experiment[10(5+3+2) Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BhBJKsdB_Jq"
   },
   "source": [
    "### a. Properly train and tune a collection of random forest classifiers using cross-validation for the design matrices X1 and X2. You should end up with two classifiers, M1 and M2. Print the best f1_scores on the test set for both the design matrices.[8 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USTbwme0HkZF"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "SXt7Bh5pv62g",
    "outputId": "21b0c689-1640-4094-bb69-9b51d1ea34c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Get the value for y from the original dataframe\n",
    "y_train = train['sentiment'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWbr-MZgwI_4"
   },
   "outputs": [],
   "source": [
    "def split(X, label):\n",
    "    \"\"\"Helper function to create a train-test split of the data.(test_size = 0.2, random_state = 0)\n",
    "\n",
    "     Arg: X: Design Matrix(ndarray)\n",
    "          label: sentiment values\n",
    "     Returns:\n",
    "          x_train: train input features\n",
    "          x_test: test input features\n",
    "          y_train: train labels\n",
    "          y_test: test labels\n",
    "    \"\"\"\n",
    "    ###Add your code here.\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, label, test_size = 0.2, random_state = 0)\n",
    "   \n",
    "    #####################\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPD5H-qcwRxD"
   },
   "outputs": [],
   "source": [
    "# Call split function for both the design matrices x1 and x2 obtained from Q1. \n",
    "###Add your code here.\n",
    "x1_train_split, x1_test_split, y1_train_split, y1_test_split = split(x1_train, y_train)\n",
    "x2_train_split, x2_test_split, y2_train_split, y2_test_split = split(x2_train, y_train) \n",
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xBnc29crwU7M"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\ptbut\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def cross_val(x_train_split,y_train_split,n_estimators):\n",
    "    \"\"\"function to calculate the cross validation scores for the 2 values of x_train, and y_train on the Random \n",
    "        Forest Classifier.\n",
    "\n",
    "     Arg: x_train_split: train input features\n",
    "          y_train_split: train labels\n",
    "          n_estimators: List of estimators to perform Random Forest with.\n",
    "     Returns:\n",
    "          cross_val_scores: List of the CV scores the estimators (list)\n",
    "    \"\"\"\n",
    "    \n",
    "    ###Add your code here.\n",
    "    \n",
    "    cross_val_scores = []\n",
    "    for n in n_estimators:\n",
    "        model = RandomForestClassifier(n_estimators=n)\n",
    "        cross_val_scores.append(cross_val_score(model, x_train_split, y_train_split, scoring=\"f1\"))\n",
    "    \n",
    "    #####################\n",
    "    return cross_val_scores\n",
    "n_estimators = [50, 100, 150, 200, 300, 400, 500, 750, 1000]\n",
    "result1 = cross_val(x1_train_split,y1_train_split,n_estimators) #CV scores for X1\n",
    "result2 = cross_val(x2_train_split,y2_train_split,n_estimators) #CV scores for X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "yUt2c_uswkJX",
    "outputId": "aa416b5d-b38b-4ae1-e897-55572431d7e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best f1 score using X1 as input is 0.6291192330736968\n",
      "The best f1 score using X2 as input is 0.6274912677213889\n"
     ]
    }
   ],
   "source": [
    "def cal_f1(x_train_split,y_train_split,x_test_split,y_test_split,result,n_estimators):\n",
    "    \"\"\"function to calculate the best F-1 score based on the best value of n_estimators obtained from cross-val.\n",
    "\n",
    "     Arg: x_train_split: train input features\n",
    "          y_train_split: train labels\n",
    "          x_test_split: test input features\n",
    "          y_test_split: test labels\n",
    "          n_estimators: List of estimators to perform Random Forest with.\n",
    "     Returns:\n",
    "          f1: f1_score for the best value of n_estimator obtained from cross-val (float)\n",
    "    \"\"\"\n",
    "    ###Add your code here.\n",
    "    n_best = n_estimators[np.argmax(np.mean(result, axis=1))]\n",
    "    model = RandomForestClassifier(n_estimators = n_best)\n",
    "    model.fit(x_train_split, y_train_split)\n",
    "    preds = model.predict(x_test_split)\n",
    "    f1 = f1_score(preds, y_test_split)\n",
    "    #####################\n",
    "    return f1\n",
    "f1_first = cal_f1(x1_train_split,y1_train_split,x1_test_split,y1_test_split,result1,n_estimators)\n",
    "f1_second = cal_f1(x2_train_split,y2_train_split,x2_test_split,y2_test_split,result2,n_estimators)\n",
    "print(\"The best f1 score using X1 as input is {0}\".format(f1_first))\n",
    "print(\"The best f1 score using X2 as input is {0}\".format(f1_second))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "isKMNQyTCZIT"
   },
   "source": [
    "### b. Which featurization technique works best for sentiment classification? Is this better or worse than the simple bag-of-words approach? What are at least three things you could do to improve the efficacy of the classifier?[2 Points]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGlmBKgFCaSp"
   },
   "source": [
    "Word2Vec seems to perform a bit better than bag of words for sentiment analysis, although they gave comparable results in terms of f1 score. The classification procedure has several steps thay may be improved to make the ofverall classifier better:\n",
    "1. The embedding using using our sentences seems to work a little better than the pre-defined google model. This embedding can be further improved by properly tuning the parameters (embedding dimension, min words, context, etc).\n",
    "2. We only try one value for number of clusters in k-means. We could tune this parameter as well.\n",
    "3. Random Forest may be overfit. We could try using further constraints (i.e. max tree depth) to make the classifier itself perform better."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW2_CSE_6240_Template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
